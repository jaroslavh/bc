% arara: xelatex
% arara: xelatex
% arara: xelatex


% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=B,english]{FITthesis}[2012/10/20]

\usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8
% \usepackage[latin2]{inputenc} % LaTeX source encoded as ISO-8859-2
% \usepackage[cp1250]{inputenc} % LaTeX source encoded as Windows-1250

\usepackage{graphicx} %graphics files inclusion
% \usepackage{subfig} %subfigures
% \usepackage{amsmath} %advanced maths
% \usepackage{amssymb} %additional math symbols

\usepackage{dirtree} %directory tree visualisation
\usepackage{color} %VH for comments
\usepackage{csquotes}

\usepackage{todonotes}

\usepackage{amssymb}

\usepackage{algorithm, algcompatible} %for for creating algorithm figures
\algnewcommand\INPUT{\item[\textbf{Input:}]}
\algnewcommand\OUTPUT{\item[\textbf{Output:}]}
\algdef{SE}[DOWHILE]{DO}{DOWHILE}{\algorithmicdo}[1]{\algorithmicwhile\ #1}%

\newcommand{\specialcell}[2][l]{%
\begin{tabular}[#1]{@{}l@{}}#2\end{tabular}}

%\usepackage[main=english,czech]{babel}

% % list of acronyms
% \usepackage[acronym,nonumberlist,toc,numberedsection=autolabel]{glossaries}
% \iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
% \makeglossaries

% ------------- Colors abbreviations ------------
\def\cbl{\color{blue}} %VH's suggestions
\def\cre{\color{red}}  %JH's discussion
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %
% EDIT THIS
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % %

\department{Department of Computer Systems}
\title{ Memory efficient cluster representations in non-metric spaces}
\authorGN{Jaroslav} %author's given name/names
\authorFN{Hlaváč} %author's surname
\author{Jaroslav Hlaváč} %author's name without academic degrees
\authorWithDegrees{Jaroslav Hlaváč} %author's name with academic degrees
\supervisor{Ing. Martin Kopp}
\acknowledgements{THANKS (remove entirely in case you do not with to thank anyone)} %TODO
\abstractEN{Summarize the contents and contribution of your work in a few sentences in English language.}
\abstractCS{V n{\v e}kolika v{\v e}t{\' a}ch shr{\v n}te obsah a p{\v r}{\' i}nos t{\' e}to pr{\' a}ce v {\v c}esk{\' e}m jazyce.} %TODO
\placeForDeclarationOfAuthenticity{Prague}
\keywordsCS{Výběr reprezentantů, Detekce anomálií, Komunitní clusterování, Systém detekce narušení, $\delta$-medoids, Topologický prostor, Párová podobnost} %TODO
\keywordsEN{Representative selection, Network anomaly detection, Community-based clustering, Intrusion detection system, $\delta$-medoids, Topological space, Pair-wise similarity} %TODO
\declarationOfAuthenticityOption{1} %select as appropriate, according to the desired license (integer 1-6)
% \website{http://site.example/thesis} %optional thesis URL


\begin{document}

% \newacronym{CVUT}{{\v C}VUT}{{\v C}esk{\' e} vysok{\' e} u{\v c}en{\' i} technick{\' e} v Praze}
% \newacronym{FIT}{FIT}{Fakulta informa{\v c}n{\' i}ch technologi{\' i}}

\setsecnumdepth{part}
\chapter{Introduction}

The Internet became the dominant medium for communication several decades age, and there are still many issues concerning its security.
Companies all around the world connect more and more of their infrastructure to the Internet.
Each connected device is an opportunity for attackers.
Security engineers are always inventing new methods to stop the attacks and keep their networks safe and secure.
It is a never-ending race between attackers, trying to overcome new security methods, and defenders, the security personnel, trying to be always one step ahead of the attackers.
This race creates the drive for more and more sophisticated defense mechanisms.

Multi-layer defense systems are protecting contemporary networks.
As attackers overcome one layer, there is yet another layer waiting for them.
However, if the attackers succeed to bypass all of them unnoticed, a huge problem can occur.
Attackers can collect user credentials,  exfiltrate data or gain control of devices without anybody's knowledge.
It can take a long time before somebody notices the breach.
A similar thing happened in Marriot International child company Starwood in 2018 \cite{hron2018breaches}.
Names, emails, addresses and credit card numbers of 500 million customers were stolen in an attack that was discovered in 2018 but possibly could have begun as early as in 2014.

One of several methods that are used to detect such intrusions is looking for anomalies in the behavior of network hosts - Network Behavioral Anomaly Detection (NBAD).
In NBAD, devices inside of the network are used as probes to gather information about the behavior of the network or individual hosts.
The current behavior of the network is then compared to a model created from the historical data.
If a significant deviation occurs, the security team is notified and can act accordingly.

It is not uncommon for a company to have tens of thousands of devices and even more.
Using a single model for the whole network may be insufficient to detect breaches successfully.
For example, an infection of a single device can be missed in the traffic of a medium sized company.
In order to detect such anomalies, it is needed to focus on smaller parts of the network.
Behavioral clustering of network hosts can be used to divide it into smaller pieces.

In Cognitive Targeted Anomaly Detection Framework from Cisco, behavioral clustering is used to find groups of similar hosts in a network.
The main focus of this thesis is to find a memory efficient representation of these clusters.
Terms representation of a cluster and cluster prototype are synonymous and are used interchangeably in this thesis. \todo{MK: is this better about cluster prototypes?}
These cluster prototypes are then updated dynamically as the behavior of the network changes.
Four algorithms previously used to represent clusters were studied and modified for usage in Cognitive anomaly detection framework.
They were tested and compared on both real and test datasets.

The thesis is organized as follows.
Chapter~\ref{ch:ids} introduces the anomaly-based network security.
Chapter~\ref{ch:theory} explains the theory behind clustering and representative selection in metric and non-metric spaces.
Chapter~\ref{ch:datasets} presents datasets used for testing.
Chapter~\ref{ch:experiments} covers all the experiments with their results and is followed by the conclusion of this thesis.

%========================================CHAPTER==================================

\chapter{Goals}

This work aims to study methods used for the representation of network host behavioral clusters that can be applied in Cognitive Targeted Anomaly Detection Framework.
Behavioral similarity used for clustering in this framework does not form a metric space, which creates a requirement for algorithms used for representative selection.

This thesis should present a method that can be used for finding the cluster prototypes of clusters in a non-metric space.
Cluster prototypes represent the whole cluster in a way that is convenient for further computation.
It can be, for example, assigning newly added samples to their corresponding cluster.

Networks monitored by Cognitive Targeted Anomaly Detection Framework can have up to several hundreds of thousands of hosts which leads to massive clusters.
If the classification of newly added hosts were done by comparing the new host to all others, the memory and time needed for classification would be unmanageable.
In the framework, each cluster is represented by a subset of its samples.
The number of representatives selected from each cluster should be kept as small as possible to maintain computational and memory efficiency.

Methods used for general cluster representation in non-metric spaces should be studied.
From these previously studied methods, the best-suited ones will be selected and implemented.
As part of this thesis, a benchmark dataset for testing the chosen methods should also be created.
A further goal is to test the chosen methods on clustering datasets used by others and on a newly created benchmark dataset.
Based on this testing, a method should be found that suits best the security field and can be incorporated into the Cognitive Targeted Anomaly Detection Framework.
The selected method should be finetuned to get the best possible results in anomaly-detection.

\setsecnumdepth{all}

%========================================CHAPTER==================================

\chapter{Intrusion Detection System}\label{ch:ids}

This chapter focuses on presenting ideas from network security needed to understand the main objective of this thesis.
Different approaches to network intrusion detection systems (IDS) are explained in Section~\ref{sec:ids}.
In Section~\ref{sec:nbad}, the scope is narrowed to network anomaly detection IDS and Section~\ref{sec:challenges}delves deeper into the challenges of it.
Section~\ref{sec:ctadf} serves as an introduction to Cognitive Targeted Anomaly Detection Framework used by Cisco.
Methods researched in this thesis were tested on real data collected from Cognitive Targeted Anomaly Detection Framework.

\section{Types of Intrusion Detection Systems}\label{sec:ids}

An intrusion detection system (IDS) is a security tool designed for identification of unauthorized use or abuse of computer systems by both system insiders and external penetrators \cite{mukherjee1994network}.
IDS that is explicitly designed for monitoring computer network is called Network IDS (NIDS) as in comparison to host IDS and hybrid IDS.
Host IDS focuses on monitoring the internal state of a computer system (e. g. mainframe computer) and its dynamic behavior.
Hybrid IDS combines different techniques including the ones used in a host and network IDS creating an IDS that can leverage information from each of its parts for better intrusion detection.
Examples of NIDS software are \cite{cooper2019bestids} :
\begin{itemize}
    \item Snort
    \item Suricata
    \item Bro Network Security Monitor
\end{itemize}
These systems are used on computer networks to detect and sometimes even prevent attacks that are threats to one of the essential services of such a network.
These basic services are\cite{mukherjee1994network}:
\begin{itemize}
    \item Data confidentiality
    \item Data and communication integrity
    \item Accessibility
\end{itemize}
Attackers try to disrupt these services by accessing confidential information (snooping), manipulating information (data tampering attacks) or disabling access to network services (Denial of Service attacks).
IDS must have multiple components to detect as many of these attacks as possible.
As each kind of intrusion is better detected by a different method, there are several types of IDS \cite{liao2013intrusion}:
\begin{itemize}
    \item \textbf{Signature-based} (knowledge-based): Patterns of known attacks or threats are being compared to captured events for intrusion detection.
    \item \textbf{Anomaly-based} (behavior-based): A static or dynamic model of a given network is created over a period of time, and current network behavior is compared to expected (model) behavior for anomalies.
    \item \textbf{Stateful protocol analysis} (specification-based): The system keeps track of known protocols (e. g., pairing requests with replies) and finding unexpected behavior in these protocols.
\end{itemize}
Cognitive Targeted Anomaly Detection Framework combines all of these approaches.
This thesis focuses on the behavior-based part of the framework.

\section{Network Anomaly Detection}\label{sec:nbad}
Network anomaly detection is a method used in Intrusion Detection Systems (IDS) as explained in the previous section.
This method focuses on comparing network host behavior changes in time.
If a change more significant than a certain threshold is observed, network anomaly is detected.
In IDS, when an anomaly is detected, an alert is created to inform the network administrator about what has happened.
This alert can be any kind of message ranging from a syslog message to an email sent to the admin.

Host behavior is collected from devices in the network.
There is at least one device (although many times multiple) dedicated for collection of network flows (using NetFlow protocol).
Flow collection is the most widely used method for gathering data on the network.
One network flow is an aggregated information about one connection that consists of source and destination addresses, source, and destination ports, begin and end timestamps for communication and size of data transferred in each direction \cite{rfc2722}.
This aggregation of information enables much faster (even real-time) detection of problems on a network as compared to, for example, deep packet inspection (DPI).
DPI is another method used to detect problems in a network.
It focuses on exploring the payload of each packet and is mostly considered unusable.
Not only the majority of traffic is encrypted, but it is also impossible to look into each packet because of the enormous amount of traffic in today's networks.

Network flows are collected and then sent via NetFlow protocol to one place where they are stored, and evaluated by detection algorithms.
Network anomaly detection system creates a baseline model of the network behavior, which is then compared to the actual traffic.
Any sudden change is considered to be an anomaly that is reported by the IDS.

\section{Challenges of Real-Time Anomaly Detection}\label{sec:challenges}
Even if it was possible to capture all the traffic in the world and use it for anomaly detection, it would still be impossible to achieve 100\% precision.
Not every network host generates enough traffic to create a precise model of its behavior.
Thankfully absolute precision is not the goal of anomaly detection, as it serves as one of many layers of security in computer systems.
\todo{Explain better, what is the goal of anomaly detection, Grill Thesis - toto ma ted malou prioritu}

That is why models are calculated for whole networks where it is possible to create a model that reflects the behavior of the network.
This method is widely used, and on smaller (several devices) networks it is very efficient.
However, capturing flow data from a big network of thousands of devices can result in too much information.
One can easily miss one infected device generating different traffic.
The noise in the collected data is too high.

To give an example, imagine detection of a single infected device, such as a printer in a 10 story office building.
This printer, being a part of a botnet, was ordered to generate traffic for DDoS (Distributed Denial of Service) attacks.
Looking at the traffic of the printer before and after infection it could easily be seen that it increased by several hundred or even thousands of percent.
But an anomaly detection system could hardly notice a change when considering the traffic of the whole office building.

If an AD system does not mark malicious traffic as an anomaly, it is considered a false negative.
That is another challenge AD systems are struggling with, to keep the number of missed attacks at 0 or as close to it as possible.

One more problem connected to false negatives are the false positives.
A false positive is a network sample of benign network traffic marked as anomalous by the AD system.
For example, if we would have our anomaly detection system set up wrong, it might detect an anomaly every day at 8 AM when workers come to the office, start their computers and download daily email.
Comparing the time window from before 8 AM and after 8 AM would not give us any relevant information.
This problem can be mitigated by setting a window, for which the model is calculated, long enough so that these mistakes do not happen.
There are many other situations when false positive alerts can happen.
Having more than 0 false positives is not that big of a problem as having more than 0 false negatives.
In IDS there is always a way to filter and double check the anomalies.
However,  a network anomaly detection system should avoid a big overhead in false positives as it could overwhelm the system.

In conclusion, a good anomaly detection system should detect all anomalies that are somehow connected to malicious behavior while trying to keep the number of false positives alerts as small as possible.
The small number of false positives that are reported is then double-checked in the following layers in the IDS.

\section{Cognitive Targeted Anomaly Detection Framework}\label{sec:ctadf}

Algorithms studied in this thesis are tested as a part of Cognitive Targeted Anomaly Detection Framework which is a part of Cognitive Threat Analytics developed and used by Cisco.
This framework successfully uses community-based clustering on the behavior of each host \cite{kopp2018community}.
The aim is to split the whole network into smaller groups.
Running anomaly detection for each group then yields significantly better results as compared to the traditional whole network approach.
Not only it works better because of community-based clustering, but also because of the ability to adapt dynamically as the network changes.
Thus it is able to incorporate changes that happen on a given network such as adding and removing devices.

The method is separated into two phases.
In the initial phase, the state of the network is learned, and an initial model is created.
Then, in the second ongoing phase, the framework dynamically adjusts clusters to the current state of the network.

The initial phase starts by collecting 24 hours of traffic from a given network.
Collected data consists of network traffic flows and proxy server logs.
Once the 24-hour period is over, clustering of hosts starts.
Each host is represented by a tuple $h = {S^h, F^h}$, where:
\begin{itemize}
    \item $S^h$ represents set of all visited pairs server:port
    \item $F^h$ represents the frequency of visits of server:port pairs
\end{itemize}
The frequency is defined as:
\begin{equation}
F_s^h = \frac{i}{n}\sum\limits_{1}{n}I(t_i, s, h)\:,
\end{equation}
where $n$ is the number of time windows, $I$ is the indicator function, which is 1 if the network host $h$ visited the server $s$ in the timewindow $t_i$ and 0 otherwise.
This ratio-based frequency ensures that frequently visited servers (e.g., Google, Facebook) do not overshadow the less frequently visited servers.

When each host is represented, the clustering algorithm is started.
A technique called community-based clustering is used to create groups of hosts.
This clustering works for graphs, where communities are more densely connected parts of the graph.
In the words of this NBAD: Those hosts that communicate with similar peers are considered to be in the same community.
Community-based clustering is explained in detail in Section~\ref{sec:cluster_topo}.

The density of samples in sets of data is determined by cosine similarity of frequency vectors of two hosts.
Similarity measure between hosts \textit{a} and \textit{b} is defined as:
\begin{equation}
\textrm{cos}(a, b) = \frac{\sum\limits_{s \in S} F_s^a F_s^b} {\sqrt{\sum\limits_{s \in S} (F_s^a)^2} \sqrt{\sum\limits_{s \in S} (F_s^b)^2}}\:,
\end{equation}
where $F_a$, $F_b$ represent the frequency and $S$ represents the union of sets of servers visited by the network hosts $a$ and $b$.

After the clustering is done, only clusters that are bigger than 10 hosts are selected for representative selection.
Smaller clusters are analyzed using a fallback method - host-centered anomaly detection.
For the purpose of this work, we do not consider clusters smaller than 10 hosts.

When clusters are determined, $\kappa$ random representatives are selected from each cluster.
Currently, $\kappa$ is an empirically set parameter.
In this thesis, tests were made to improve this random selection method.
A more detailed explanation of this method can be found in \cite{kopp2018community}.

After the initial training phase is over and the model is established, data continues to be collected.
Every 4 hours all hosts that are observed are assigned to existing clusters.
At that point, a portion of cluster representatives is replaced by new ones to capture the ever-changing nature of the network data.

These clusters are then further used in anomaly detection.
They serve as a foundation for calculating baseline behavior for hosts belonging to it.
If a host is known to belong to a cluster $A$ and its behavior suddenly starts to differ from the behavior of representatives selected for cluster $A$, an anomaly is found and reported further into the NIDS.

%========================================CHAPTER==================================

\chapter{Theoretical Overview and State of the Art}\label{ch:theory}

This chapter introduces a theoretical background to explain clustering methods, the topic of non-metric spaces and the problem of representative selection.

\section{Introduction to Clustering}\label{sec:clust_intro}
As defined in \cite{guttag2016introduction}, clustering is an unsupervised machine learning method that organizes objects into groups so that each group consists of members that are similar in some way.
Without any prior knowledge of the data, this method looks for structures in feature vectors.
Variability can be calculated for each cluster \textit{c}, and it shows how much objects in given cluster differ.
Variability and dissimilarity are two properties defined for a better understanding of the data.
Variability is defined as
\begin{equation}
\mathrm{variability}(c) = \sum_{e \in c} \mathrm{distance}(\mathrm{mean}(c), e)^2\:,
\end{equation}
\todo{are these equation numbers-corresponding to sections correct?} where \textit{e} is an object from given cluster.
The distance is a measure that quantifies the proximity of two objects with the same number of features.
Many different distance measures are used in clustering.
Several of commonly used ones are:
\begin{itemize}
    \item Euclidean distance
    \item Minkowski distance
    \item Manhattan distance
    \item Cosine similarity
\end{itemize}
Cosine similarity measures the angle between two feature vectors.

Dissimilarity is defined as
\begin{equation}
\mathrm{dissimilarity}(C) = \sum_{c \in C} \mathrm{variability}(c)\:,
\end{equation}
where $C$ stands for all clusters the data was clustered into.
For clustering, the aim is to keep the dissimilarity of all clusters from the dataset as low as possible.
Given this definition, the best way to cluster every dataset would be to put each object to its cluster.
That would not lead to any reasonable result.
Therefore there is a constraint added for clustering methods.
It can be either the maximum number of clusters or the maximum distance between two clusters.

A straightforward example of clustering is a method called agglomerative hierarchical clustering.
Given $N$ objects in a dataset, it creates $N$ clusters - meaning there is a cluster for each object.
The method looks for two closest clusters and merges them into one.
This agglomerative merging continues until the constraint is met, meaning until there is a certain number of clusters or until the distance between closest clusters exceeds a certain threshold.
This method is a greedy algorithm, and therefore it might not result in globally optimal clustering.
Also, the algorithm has a time complexity of $\mathcal{O}(n^2)$.
Therefore, it cannot be used in big datasets.

An example of a much faster clustering algorithm that is also greedy is $K$-means.
The '$K$' in $K$-means stands for the number of clusters that we want to get as a result.
To use this algorithm the number of desired clusters has to be known in advance.
$K$-means randomly chooses $K$ centroids in the space of the dataset and then assigns each point to a centroid.
After creating these clusters, it calculates a new centroid for each cluster and then assigns the points in datasets to the new centroids.
The algorithm stops when the centroids of clusters stop changing.
Figure~\ref{alg:kmeans} shows the pseudocode of $K$-means.
\begin{algorithm}[H]\label{alg:kmeans}
    \caption{K-Means}
    \label{k_mean_pseudocode}
    \begin{algorithmic}[1]
        \INPUT data $X = x_0, x_1, ..., x_n$; number of clusters $k$
        \OUTPUT $k$ clusters; $k$ centroids
        \STATE $t = 0$
        \STATE Initialize $centroids_t$ = $k$ randomly chosen examples from $X$
        \DO
            \STATE $t = t + 1$
            \STATE Initialize $clusters = \emptyset$
            \FOR{$c$ in $centroids_t$}
                \STATE Initialize $cluster_c = \emptyset$
                \STATE add $cluster_c$ to $clusters$
            \ENDFOR
            \FOR{$x$ in $X$}
                \STATE $closest\_centroid = \mathrm{argmin}_{c \in centroids} d(c, x)$
                \STATE add $x$ to $cluster_{closest\_centroid}$
            \ENDFOR
            \STATE $centroids_t = \emptyset$
            \FOR{$cluster$ in $clusters_t$}
                \STATE $new\_centroid  = \mathrm{mean}(cluster)$
                \STATE add $new\_centroid$ to $centroids_t$
            \ENDFOR
        \DOWHILE{$centroids_t = centroids_{t-1}$}
        \STATE \textbf{return} $clusters$, $centroids_t$

    \end{algorithmic}
\end{algorithm} \todo{is this pseudocode looking better}

This algorithm is fast, it has time complexity $\mathcal{O}(k \cdot n)$, where $n$ is the number of objects in a dataset and $d$ is the computation time of a distance between two points.
It is the most common clustering algorithm as it typically converges in a few iterations.
For more details about basic clustering algorithms see \cite{guttag2016introduction}.

%summary
The clustering as explained in this section has some restrictions that make it inapplicable for anomaly detection in this work.
Firstly the number of clusters is not previously known, so choosing $K$ for $K$-means is not an option, and secondly, the distance measure used does not form a metric space.
Next section delves deeper into what it means when a measure does not form a metric space.

\section{Clustering on Topological Space}\label{sec:cluster_topo}
This section explains the difference between metric and topological space and clustering approaches for topological spaces.

A metric space, as defined in \cite{choudhary1992elements}, is a pair $(X, d)$ where $X$ is a set and $d$ is a mapping from  $X \times X \to \mathbb{R}$\todo{ad pripominka, ze by to melo byt NxN, takhle to je popsane v te citovane knize. Prijde mi, ze je to obecnejsi a drzim se toho jak to popsal ten matematik.} which satisfies the following conditions:
\begin{enumerate}
    \item [(i)] $d(x, y) \geq 0$;
    \item [(ii)] $d(x, y) = 0 \iff x = y$;
    \item [(iii)] $d(x, y) = d(y, x)$
    \item [(iv)] $d(x, z) \leq d(x, y) + d(x, z) \mathrm{for} x, y, z \in X$.
\end{enumerate}

The similarity measure that is used in this thesis as explained in Section~\ref{sec:ctadf} does not fulfill the last point of the definition above. Furthermore, it is a pairwise similarity that forms form a subspace for comparing each pair of samples.
This is why this similarity measure forms a topological space, which is defined according to, e.g., \cite{stahl2014introduction}.

Given any set $S$ a topology on $S$ is a family $F ={F_{\alpha} | \alpha \in A}$, where $A$ is some indexing set, each $F_{\alpha}  \subseteq S$\todo{ad poznamka proc je to tady vsunute - je to prima citace z te knihy}, and with the following properties:
\begin{enumerate}
    \item [(i)] The empty set $\emptyset$ is in $F$.
    \item [(ii)] The given set $S$ is in $F$.
    \item [(iii)] The intersection of any two sets of $F$ is in $F$.
    \item [(iv)] The union of any number of sets of $F$ is in $F$.
\end{enumerate}
The ordered pair ($S, F$) is called a topological space.

\todo{explain about neighborhoods??}

K-Means clustering from the previous section cannot be used in topological space as the centroids do not exist.
In many cases, there is no prior knowledge of k, as is the case in this thesis.
A popular method that is used for clustering in non-metric spaces is called community-based clustering.

Community-based clustering detects communities in the data.
A community is a subset of examples in the data, that is densely connected with each other.
One of the ways how to detect communities in a graph is to create a full-adjacency matrix.
This matrix contains all connections between all nodes in the given graph.
Analyzing the matrix then can tell us about the densities in different parts of graphs.

Louvain method is a similar approach to clustering when we do not have a simple graph but a set of samples and a pairwise similarity measure.
This method relies on creating a full similarity matrix for the whole dataset and then looking for communities in the data.

In Cognitive Targeted Anomaly Detection Framework Louvain method cannot be used directly as calculating the full similarity matrix has a complexity of $\mathcal(n^2)$, which is impossible to calculate for a large network.
That is why an approximative clustering method is used.
This method iteratively samples network hosts and runs the clustering algorithm on the sampled hosts.
Each iteration creates or updates cluster prototypes.
If the data in the current batch fit into a previously prototyped cluster, they are added to it, and the cluster prototype is updated.
If samples differ more than a predefined threshold, a new cluster prototype is created.

\section{Representative Selection}
\label{sec:representative_selection}

This section focuses on the idea of finding a representation of clusters.
Clustering huge datasets can result in big clusters of several tens of thousands of objects in them.
Computational operations such as assigning new objects to clusters (e. g. $K$-Nearest Neighbors) are dependent on the number of objects in each cluster or the representation of these clusters.
If it was possible to represent these clusters in a different way than keeping all track of all of the objects, further operations on these clusters would run faster.

A cluster prototype is a data sample that represents all samples in the data cluster.
According to \ref{tan2014introduction}, there are three motivations for finding the most representative cluster prototypes:
\begin{itemize}
    \item Summarization
    \item Compression
    \item Efficient Finding Nearest Neighbors
\end{itemize}
All of these apply to the NBAD aswell.
Therefore, summarizing the behavior of the whole cluster into a cluster prototype is desirable.
As is finding the smallest possible number of prototypes for each cluster for efficient finding nearest neighbors when adding new hosts to their corresponding clusters.

\subsection{Definition of Representative Selection for Non-Metric Spaces}\label{sec:def_rep_selection}
Representative selection aims to find a minimal subset of examples from a cluster, that carries sufficient information about the whole cluster.
This problem was well defined in \cite{liebman2015representative} and the following definition is taken from that paper.

Let $S$ be a data set, $d : S \times S \to \mathbb{R} +$ be a distance measure (not necessarily a metric), and $\delta$ be a distance threshold below which samples are considered sufficiently similar.
The task is finding a representative subset $Z \subset S$ that best encapsulates the data.
Two following requirements are imposed on an algorithm for finding a representative subset:
\begin{itemize}
    \item \textbf{Requirement 1}: The algorithm must return a subset $Z \subset S$ such that for any sample $x \in S$, there exists a sample $z \in C$ satisfying $d(x, z) \le \delta$.
    \item \textbf{Requirement 2} : The algorithm cannot rely on a metric representation of the samples in $S$.
\end{itemize}
To compare the quality of different subsets returned by different algorithms, two criteria are measured:
\begin{itemize}
    \item \textbf{Criterion 1}: $|Z|$ - seeking the smallest possible subset $Z$ that satisfies Requirement 1.
    \item \textbf{Criterion 2}: Representative should best fit the data on average. Given representative subsets of equal size, the preference is on the one that minimizes the average distance of samples from their respective representatives.
\end{itemize}
Criteria 1 and 2 are applied to a representative set solution.
In addition, the following desiderata for a representative selection algorithm are expected.
\begin{itemize}
    \item \textbf{Desideratum 1}: Stable representative selection algorithms are preferred. Let $Z_1$ and $Z_2$ be different representative subsets for dataset $S$ obtained by two different runs of the same algorithm.
Stability is defined as the overlap $\frac{|Z_1 \cap Z_2|}{|Z_1 \cup Z_2|}$      The higher the expected overlap is, the more stable the algorithm is.
This desideratum ensures the representative set is robust to randomization in data ordering or the choices made by the algorithm.
    \item \textbf{Desideratum 2}: The algorithm should be efficient and scale
well for large datasets.
\end{itemize}

This definition of representative selection problem serves well for this paper.

\subsection{Representative Selection in a Metric Space}\label{sec:rep_select_metric}
A metric dataset has to have the same number of features for each sample and a metric that fulfills all the requisites in the definition of metric space.
Selecting a prototype from this dataset is often best achieved by calculating a centroid for a given cluster.
A centroid can be calculated as the mean of the points in the cluster.
An example of a centroid in a cluster can be found in Figure~\ref{img:centroids} a).
Other ways of calculating medoids can be used, e.g. weighted average of all points.

\begin{figure}
  \includegraphics[width=\linewidth]{img/centroids.png}
  \caption{Centroids of a cluster calculated as mean of all points.}
  \label{img:centroids}
\end{figure}

However, if a calculated centroid is not meaningful (see Figure~\ref{img:centroids} b)), a medoid can be selected as an alternative to it.
A medoid is a point that is in the set that minimizes the average distance to all the other points in the set.
It can be thought of as a median of the dataset \todo{this is good for an explanation, but not very scientific, as it is not entirely true, how to explain better?}.
Formally, medoid is defined as:
\begin{equation}
x_{medoid} = \mathrm{argmin}_{y \in \{x_1, x_2, ..., x_n\}} \sum_{i=1}^{n}{d(y, x_i)}\:,
\end{equation}
where $x_1, x_2, ..., x_n$ is a set of $n$ points in a space with a distance function $d$. \todo{definition taken from wikipedia - similar is in xu2011network but not that nice - how to cite?? }
In Figure~\ref{img:medoids} medoids were chosen instead of centroids.
\begin{figure}
  \includegraphics[width=\linewidth]{img/medoids.png}
  \caption{Choice of medoids}
  \label{img:medoids}
\end{figure}

\subsection{Representative Selection in a Topological Space}\label{sec:rep_select_arbitrary}
Clearly, the concept of centroid explained in the previous section cannot be used to solve the problem of finding a representative in an arbitrary space (i.e., topological space).
Medoids, of course, can be selected for datasets in arbitrary spaces.

$K$-Medoids algorithm that uses the concept of medoids instead of centroids that are used in the $K$-Means algorithm while retaining its good properties.
$K$-Medoids is most commonly used for representative selection in topological spaces.

There are not many more well-explored methods for solving this problem.
The one that is used in this thesis is explained in \cite{liebman2015representative}.
They took the main ideas of $K$-Means clustering and transformed them for usage in a non-metric space with a pair-wise similarity measure.
Instead of stating the $K$ in advance they state a parameter  $ 0 \leq \delta \leq 1$ that serves as a constraint.
Then they separate the cluster into subclusters based on this parameter.
Each of these subclusters is represented by a medoid.
A set of these medoids then serves as a cluster prototype for the whole cluster.
This method is explained in greater detail in the Section~\ref{subsec:delta_medoids}.

\section{Algorithms Relevant for Topological Space}\label{sec:relevant_methods}
Based on the previously explained approaches for finding representatives of clusters in non-metric, spaces the following methods were chosen, tested and compared as a part of this thesis.
\begin{itemize}
    \item Random selection
    \item Greedy Selection
    \item $\delta$-Medoids One-Shot
    \item $\delta$-Medoids
\end{itemize}

\subsection{Random Selection}\label{subsec:random_select}

Random selection algorithm serves as a baseline to measure the improvement as it is the method currently used in Cognitive Targeted Anomaly Detection Framework.
It selects a given number of representatives from the cluster randomly.

In clusters dense clusters, selecting random samples from each cluster leads to undesired noise.
However as experimental results presented in \cite{kopp2018community} in the domain of computer networks the density of clusters is not significant.
It, of course, depends on each network, but given the best practices in segmentation of networks, the overlaps in clusters are not expected to be great.

For this thesis, the random selection algorithm serves as a baseline in comparing the results to other methods.

\subsection{Greedy Selection}\label{subsec:greedy_select}
\todo{obrazky sem a do dalsi sekce}
The greedy selection algorithm is a straightforward algorithm that selects a random sample from the set as the first representative.
Then it looks for the most different sample in the rest of the dataset.
Once the desired sample is found, the algorithm removes all neighboring samples with at least the similarity of $\delta$ from the set.
Then it looks for other dissimilar samples until the whole set is represented.

The speed of this algorithm depends on the sparsity of the given set.
In the worst case scenario, all points would be selected as representatives while always calculating the distance to each remaining sample in the set. Then it would reach the time complexity of $\mathcal{O}(n^2)$.

\subsection{$\delta$-Medoids}\label{subsec:delta_medoids}
The two following algorithms were first eplained in \cite{liebman2015representative}.
They were tested on two different problems that also use clustering in non-metric space - computing distance of two musical segments and comparing trajectories of objects.

$\delta$-Medoids algorithm tries to find a minimal subset of points that are needed to cover the cluster with only one given constraint, that distance of the representatives would be at least $\delta$.
There are two versions of this algorithm.
One-shot version is faster but less precise than the full $\delta$-medoids algorithm.

\subsection{$\delta$-Medoids One-Shot}

The idea behind this algorithm is to go through the dataset looking for representatives that differ at least by the given parameter $\delta$ from each previously selected representatives.
By taking this approach, it can in one iteration over data find the representatives that are certain to differ by $\delta$ from each other.

The algorithm is shown in Figure~\ref{alg:delta_medoids_one_shot}, this algorithm is optimized for better memory efficiency as opposed to the version in the cited paper.
The main ideas remain the same, only keeping track of clusters that the points are assigned to is removed for this one-shot version.

\begin{algorithm}
    \caption{$\delta$-Medoids One-shot}
    \label{alg:delta_medoids_one_shot}
    \begin{algorithmic}[1]
        \INPUT data $x_0$ ... $x_m$, required distance $\delta$
        \STATE Initialize \textit{representatives} = $\emptyset$
        \FOR{$i = 0$ \textbf{to} $m$}
            \STATE Initialize \textit{dist} = $\infty$
            \FOR{\textit{rep} in \textit{representatives}}
                \IF{$d(x_i, rep) \le dist$}
                    \STATE $dist = d(x_i, rep)$
                \ENDIF
            \ENDFOR
            \IF{$dist > \delta$}
                \STATE add $x_i$ to \textit{representatives}
            \ENDIF
        \ENDFOR
    \end{algorithmic}
\end{algorithm}


The choice of the first representative strongly influences the selection of the medoids to represent the cluster.
In some cases, this one-shot approach could be misleading.

\subsection{$\delta$-Medoids Full}

The full version of the algorithm runs a one-shot algorithm multiple times.
Each time it goes through the dataset it selects a better medoid than before.
Once two passes through the data do not change any medoid, the algorithm stops.
This algorithm is shown in Figure~\ref{alg:delta_medoids_full}.
Original algorithm from \cite{liebman2015representative} lacks the routine \textit{ReduceClusters} on line 24.
This routine is introduced in this thesis as an improvement of the algorithm because of the algorithm properties found in Section ~\ref{sec:exp1}.

This approach is slower, as the main routine from one shot algorithm has to be run multiple times.
Experimental results of \cite{liebman2015representative} show that the algorithm converges in very few (<10) cycles.
On the other hand, it is able to select better medoids, thus getting rid of the difficult choice of the first representative to select.

\begin{algorithm}
    \caption{$\delta$-Medoids}
    \label{alg:delta_medoids_full}
    \begin{algorithmic}[1]
        \INPUT data $x_0$ ... $x_m$, required distance $\delta$
        \STATE $t = 0$
        \STATE Initialize $representatives_{t_0} = \emptyset$
        \STATE Initialize \textit{clusters} = $\emptyset$
        \DO
            \STATE $t = t + 1$
            \FOR{$i = 0$ \textbf{to} $m$}
                \STATE Initialize \textit{dist} = $\infty$
                \STATE Initialize \textit{representative = null}
                \FOR{\textit{rep} in \textit{representatives}}
                    \IF{$d(x_i, rep) \le dist$}
                        \STATE \textit{representative = rep}
                        \STATE $dist = d(x_i, rep)$
                    \ENDIF
                \ENDFOR
                \IF{$dist \le \delta$}
                    \STATE add $x_i$ to $cluster_{representative}$
                \ELSE
                    \STATE \textit{representative = $x_i$}
                    \STATE Initialize $cluster_{representative} = \emptyset$
                    \STATE add $x_i$ to $cluster_{representative}$
                    \STATE add $cluster_{representative}$ to \textit{clusters}
                \ENDIF
            \ENDFOR
            \STATE Call \textbf{\textit{ReduceClusters}}
            \STATE Initialize $representatives_t = \emptyset$
            \FOR{\textit{cluster} in \textit{clusters}}
                \STATE $representative = \textrm{argmin}_{s \in cluster} (\sum\limits_{x \in cluster}{d(x,s) : d(x,s) \le \delta)}$
                \STATE add \textit{representative} to $representatives_t$
            \ENDFOR
        \DOWHILE{$representatives_t = representatives_{t-1}$}
    \end{algorithmic}
\end{algorithm}

The subroutine \textit{ReduceClusters} shown in Figure~\ref{alg:reduce} helps to reduce the number of selected representatives to a number lower than a constant $\kappa$.
If a representative does not cover at least 1\% of the cluster, the routine tries to add him to the coverage of the representative that is similar to it.
If no such similar representative exists, it is considered noise and the representative is dropped from the cluster.

\subsection{$\delta$-Medoids Modified}

\todo{modified algorithm betterexplained}

\begin{algorithm}
    \caption{ReduceClusters}
    \label{alg:reduce}
    \begin{algorithmic}[1]
        \INPUT $m$ subclusters $X$ = $x_i, ..., x_m$; their representatives $R = r_0, ..., r_m$; whole cluster size$s$
        \STATE $threshold = 0.01s$
        \STATE Initiate $i = 0$
        \IF{$|X| > \kappa$}
            \WHILE{$i < m$}
                \IF{$|x_i| \le threshold$}
                    \STATE $j$ = index the most similar representative from $R$
                    \IF{no such $j$ exists}
                        \STATE Drop $x_i$ from $X$
                    \ELSE
                        \STATE Merge $x_i$ and $x_j$.
                    \ENDIF
                    \STATE Drop $r_i$ from $R$
                \ENDIF
            \ENDWHILE
        \ENDIF
    \end{algorithmic}
\end{algorithm}

The time complexity is of $\delta$-Medoids is shown on Figure~\ref{img:complexity}.

\begin{figure}
  \includegraphics[width=\linewidth]{img/complexity.png}
  \caption{Comparing run times of $\delta$-Medoids and Random Selection}
  \label{img:complexity}
\end{figure}

%========================================CHAPTER==================================

\chapter{Datasets}\label{ch:datasets}

The task of cluster representation in non-metric spaces generally is not necessarily connected only to the security field.
That is why, before moving to a dataset collected from the real network, the selected algorithms were first tested on artificially created datasets and well labeled datasets found in the literature.

Below, the datasets used in this thesis are presented.
There are three types of datasets used: artificially created, image recognition and real network dataset.
Each section corresponds to one of the listed types.
\todo{is this introduction enough? What is missing?}

\section{Datasets Created for this Work}

These datasets were created to test the specific properties of tested methods.
The datasets created are the following:
\begin{itemize}
    \item Blobs 3D
    \item Overlap
    \item Circles 3D
    \item Moons
\end{itemize}
All of these datasets are in space of two or three dimensions.
They can be easily visualized on graphs and nicely show some of the specific properties of implemented algorithms.
\begin{figure}
  \includegraphics[width=\linewidth]{img/datasets.png}
  \caption{Graphs of all datasets}
  \label{img:datasets}
\end{figure}
Similarity measure used in these datasets was Euclidean distance.

The Blobs datasets are very easy to represent with a centroid and were tested whether and how much points is needed to represent them using methods explained in Chapter X. \todo{Link to chapter} The Overlap dataset consists of 3 clusters in shapes of blocks.
These blocks partly overlap to test how selected algorithms cope with overlapping clusters.
Circles 3D is one of the hardest datasets to represent even by methods that are designed specifically for metric space.
These two datasets were chosen to test the resistance to noise in selecting representatives.
There are two clusters as shown in Figure~\ref{img:datasets}.

\section{Image Recognition Datasets}
One of the best-documented areas of clustering is the area of image recognition.
That is why two datasets from this area were chosen for measuring the precision and complexity of each method.
Both of them are labeled datasets of features collected from digits written by human hand.

The Pen-Based Recognition of Handwritten Digits Data Set \cite{dua1998pendigit} consists of 10922 samples with 16 attributes.
Each sample represents features collected from a handwritten digit.
Each digit is represented by a little bit more than a thousand samples.
Features represent coordinate information about the digits as they were written on a $500\times500$ pixel frame.
This dataset is split to train dataset (70\%) and test dataset (30\%).

The Multiple Features Data Set \cite{dua1998pendigit} consist of \todo{explain how is it different} \todo{add MNIST Fashion - if works remove multiple features??? Even though I have results.?}

\section{Real Network Dataset}\label{sec:real-data}
Real network traffic captured from a company with approximately 20 thousand employees.
The capture is from 24 hours of a working day a consists of \todo{count flows} flows.
Proxy logs were used to capture the connections.
Hosts that appeared in that capture were clustered using the method explained in Section~\ref{sec:cluster_topo}.
These clusters with all hosts labeled were used as real network dataset in this thesis.


%========================================CHAPTER==================================

\chapter{Experiments}\label{ch:experiments}
In this chapter each experiment that was run.
Each section is dedicated to one experiment.
For each experiment, the motivation, experimental setup, results and conclusion are listed.

All experiments were made in Jupyter Notebook technology using Python 3.7 kernel.
For loading data from files and storing them in memory was used the Pandas library version 0.23.4\todo{link to documentation}.
These technologies were chosen due to this thesis's experimental nature, as the goal was to produce a prototype solution for this problem.

\section{Experiment 1}\label{sec:exp1}\todo{name differently than Experiment?}

The first experiment was run on easily the data specifically created for this thesis.
It served as a sanity check of the proposed methods.
The methods that were explained in the previous chapter\todo{make reference} are designed to work in non-metric space but they still work in a metric space.
Using them on metric data and visualizing the results helped a lot to check the assumptions about results experimentally.
Also, the need for selection of few representatives from each dataset was tested.
It is expected, for these simple clusters, not to exceed 30 selected representatives.

\subsection{Experimental Setup}
Datasets used for this experiment were:
\begin{itemize}
    \item Blobs 3D
    \item Overlap
    \item Moons
    \item Noisy Circles 3D
\end{itemize}

All 4 algorithms were run.
The $\delta$ was estimated as \todo{how is median at lower ten percent called, put it to maths} times 1.05 from distances from one point in the data to all of the others.

\subsection{Results}

The results for each dataset are listed below.

\textbf{Blobs 3D}

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
A & 400 & 21 & 23 & 23 & 23 \\
B & 400 & 17 & 20 & 20 & 20 \\
C & 400 & 19 & 20 & 20 &20  \\ \hline
\end{tabular}
\caption{Number of selected representatives for Blobs 3D dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp1_blobs.png}
  \caption{Confusion matrices for Blobs 3D dataset}
  \label{img:exp1_blobs}
\end{figure} \todo{generate pictures with cluster names rathern then numbers, or do i have to?}

\textbf{Overlap}

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
A & 800 & 25 & 23 & 23 & 23 \\
B & 960 & 26 & 21 & 21 & 21 \\
C & 640 & 22 & 24 & 24 & 24  \\ \hline
\end{tabular}
\caption{Number of selected representatives for Overlap dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp1_overlap.png}
  \caption{Confusion matrices for Overlap dataset}
  \label{img:exp1_overlap}
\end{figure}

\textbf{Moons}

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
A & 600 & 18 & 19 & 19 & 19 \\
B & 600 & 19 & 18 & 18 & 18 \\ \hline
\end{tabular}
\caption{Number of selected representatives for Moons dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp1_moons.png}
  \caption{Confusion matrices for Moons dataset}
  \label{img:exp1_moons}
\end{figure}

\textbf{Circles}

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
A & 600 & 14 & 14 & 14 & 14 \\
B & 600 & 8 & 7 & 7 & 7 \\ \hline
\end{tabular}
\caption{Number of selected representatives for Circles dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp1_circles.png}
  \caption{Confusion matrices for Circles dataset}
  \label{img:exp1_circles}
\end{figure}


\subsection{Conclusion}
At first glance, all the confusion matrices look almost perfect and as expected.
The number of representatives selected did not exceed 30 in any cluster.
The number of missed classification is low even in overlapping or complex datasets.
However, when the results were visualized for one cluster, the delta medoids algorithm did not result in expected selection.
As can be seen on Figure~\ref{img:blobs_border_select}, the algorithm picked the samples on the edge of the dataset that did not cover many points in their neighborhoods.
\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/delta_medoids_select.png}
  \caption{$\delta$-Medoids choosing the border samples of a cluster}
  \label{img:blobs_border_select}
\end{figure}
Selecting the border samples clearly was not intended and the $\delta$-Medoids algorithm had to select only the relevant samples.
The relevance of a representative was determined to the number of points that fit into its $\delta$-neighborhood. \todo{where to list the changes, here or in the theoretical chapter, theory better I think}

\section{Experiment 2}\label{sec:exp2}

This experiment was run to test the changes made in the $\delta$-Medoids algorithm.
It was updated not to select the border samples, as is explained in section\todo{ref the section where I explain it}.

\subsection{Experimental Setup}
Datasets used for this experiment were:
\begin{itemize}
    \item Blobs 3D
    \item Overlap
    \item Moons
    \item Noisy Circles 3D
\end{itemize}

All 4 algorithms were run.
The $\delta$ was estimated as \todo{how is median at lower ten percent called, put it to maths} times 1.05 from distances from one point in the data to all of the others.

\subsection{Results}

The results for different methods are listed below.

\textbf{Blobs 3D}

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
A & 400 & 23 & 24 & 24 & 24 \\
B & 400 & 21 & 22 & 22 & 22 \\
C & 400 & 20 & 21 & 21 & 21 \\ \hline
\end{tabular}
\caption{Number of selected representatives for Blobs 3D dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp2_blobs.png}
  \caption{Confusion matrices for Blobs 3D dataset}
  \label{img:exp2_blobs}
\end{figure}

\textbf{Overlap}

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
A & 800 & 22 & 21 & 21 & 21 \\
B & 960 & 21 & 20 & 20 & 20 \\
C & 640 & 19 & 19 & 19 & 19  \\ \hline
\end{tabular}
\caption{Number of selected representatives for Overlap dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp2_overlap.png}
  \caption{Confusion matrices for Overlap dataset}
  \label{img:exp2_overlap}
\end{figure}

\textbf{Moons}

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
A & 600 & 21 & 18 & 18 & 18 \\
B & 600 & 16 & 20 & 20 & 20 \\ \hline
\end{tabular}
\caption{Number of selected representatives for Moons dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp2_moons.png}
  \caption{Confusion matrices for Moons dataset}
  \label{img:exp2_moons}
\end{figure}

\textbf{Circles}

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
A & 600 & 15 & 15 & 15 & 15 \\
B & 600 & 8 & 8 & 8 & 8 \\ \hline
\end{tabular}
\caption{Number of selected representatives for Circles dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp2_circles.png}
  \caption{Confusion matrices for Circles dataset}
  \label{img:exp2_circles}
\end{figure}


\subsection{Conclusion}

The results of this experiment empirically supported the expected result of not selecting the border samples from a dataset as can be seen on Figure~\ref{img:blobs_good_select}
\begin{figure}[H]
  \includegraphics[width=\linewidth]{img/delta_medoids_select_better.png}
  \caption{$\delta$-Medoids choosing only relevant samples of a cluster}
  \label{img:blobs_good_select}
\end{figure}
This sanity check by visualization fulfilled all of the expectation.

\section{Image Recognition Datasets}\label{sec:exp3}

After the algorithms were tested on visualizable data, the experiments with well known labeled datasets could start.
The motivation for this experiment was to test the algorithm results on well explored and well-annotated datasets commonly used as a benchmark for clustering.

\subsection{Experimental Setup}
Datasets used for this experiment were:
\begin{itemize}
    \item Pendigits
    \item MNIST Fashion
\end{itemize}

The $\delta$ was estimated as \todo{how is median at lower ten percent called, put it to maths} times 1.05 from distances from one point in the data to all of the others.

\subsection{Results}

The results for different methods are listed below.

\textbf{Pendigits}

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
0 & 780 & 33 & 33 & 33 & 33 \\
1 & 779 & 33 & 41 & 41 & 41 \\
2 & 780 & 17 & 15 & 15 & 15 \\
3 & 719 & 12 & 9 & 9 & 9 \\
4 & 780 & 25 & 24 & 24 & 24 \\
5 & 720 & 25 & 27 & 27 & 27 \\
6 & 720 & 16 & 16 & 16 & 16 \\
7 & 778 & 22 & 20 & 20 & 20 \\
8 & 719 & 75 & 77 & 15 & 15 \\
9 & 719 & 50 & 52 & 13 & 13 \\ \hline
\end{tabular}
\caption{Number of selected representatives for Pendigit dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp3_pendigits.png}
  \caption{Confusion matrices for Pendigits dataset}
  \label{img:exp3_pendigits}
\end{figure}

\textbf{MNIST Fashion}

In the experiment for this dataset the Greedy Selection algorithm was omitted as the experiments would take too long on clusters that contain 6000 samples.
Previous experiments show that the $\delta$-Medoids agorithm chooses similar amount of representatives while getting better coverage.

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
0 & 6000 & 859  & 23 & 23 \\
1 & 6000 & 176  & 16 & 16 \\
2 & 6000 & 845  & 20 & 20 \\
3 & 6000 & 616  & 22 & 22 \\
4 & 6000 & 630  & 25 & 25 \\
5 & 6000 & 1622 & 14 & 14 \\
6 & 6000 & 1181 & 18 & 18 \\
7 & 6000 & 266  & 19 & 19 \\
8 & 6000 & 1971 & 16 & 16 \\
9 & 6000 & 686  & 28 & 28 \\ \hline

\end{tabular}
\caption{Number of selected representatives for MNIST Fashion dataset}
\end{table}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp3_mnist_fashion.png}
  \caption{Confusion matrices for real network data}
  \label{img:exp4}
\end{figure} \todo{table with representatives selected}

\subsection{Conclusion}
Using algorithms on real datasets started to show more significant differences between results in algorithms.
An interesting thing is that Greedy Selection and $\delta$-Medoids One-Shot select similar number of representatives and their results are also very similar.
This is interesting as the complexity of these algorithms differs a lot, Greedy Selection being much slower with $\mathcal(n^2)$. \todo{better wording of this conclusion}

\section{Experiment 4}\label{sec:exp4}


\subsection{Experimental Setup}
For this experiment clustered hosts from a real network were used.
This dataset is explained in greater detail in Section~\ref{sec:real-data}.
From this dataset traffic for first 4 hours was used for training the model and the next 4 hours were used for testing the representatives.
The similarity measure from Section~\ref{sec:ctadf} was used.

All 4 algorithms were run.
The $\delta$ was set to 0.8 as was the threshold used in community-based clustering for their creation.

\subsection{Results}

The results for different methods are listed below.

\begin{table}[H]
\begin{tabular}{l|lllll}
\hline
 \textbf{Cluster} & \specialcell{\textbf{All}\\ \textbf{Samples}} & \specialcell{\textbf{Greedy}\\ \textbf{Selection}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{One-Shot}} & \specialcell{\textbf{$\delta$-Medoids}\\ \textbf{Full}} & \specialcell{\textbf{Random}\\ \textbf{Selection}} \\ \hline
1024 & 72   & 0 & 8   & 8  & 8  \\
865  & 89   & 0 & 28  & 28 & 28 \\
323  & 2125 & 0 & 821 & 40 & 40 \\
68   & 116  & 0 & 82  & 36 & 36 \\
645  & 83   & 0 & 56  & 56 & 56 \\
1004 & 46   & 0 & 38  & 38 & 38 \\
43   & 55   & 0 & 31  & 31 & 31 \\
1196 & 835  & 0 & 441 & 34 & 34 \\
253  & 59   & 0 & 47  & 47 & 47 \\
112  & 77   & 0 & 66  & 66 & 66 \\
946  & 2065 & 0 & 781 & 30 & 30 \\
116  & 238  & 0 & 71  & 17 & 17 \\
393  & 51   & 0 & 38  & 38 & 38 \\
756  & 127  & 0 & 92  & 34 & 34 \\
124  & 67   & 0 & 48  & 48 & 48 \\
221  & 50   & 0 & 50  & 50 & 50 \\ \hline
\end{tabular}
\caption{Number of selected representatives for real network data}
\end{table}
\todo{greedy medoids jeste pocitam}

\begin{figure}[H]
   \includegraphics[width=\linewidth]{img/exp4.png}
  \caption{Confusion matrices for real network data}
  \label{img:exp4}
\end{figure}

\todo{here add better cluster names for sure}
\todo{get precision and recall values per cluster}

\subsection{Conclusion}
The results show that the $\delta$-Medoids algorithm is significantly better than selecting random representatives from the cluster.
Even for clusters with several thousand of samples few samples were selected a while retaining good properties for assigning points to clusters.
The downside is the speed of the algorithm but this was not a part of this experiment.
\todo{expand the conclusion}


\section{Experiment 5}\label{sec:exp5}

In this experiment the average data fit as defined in Section~\ref{sec:representative_selection} was tested.

\subsection{Experimental Setup}
In this experiment one cluster was represented by $\delta$-Medoids and Random Selection algorithms.
The cluster was selected from dataset\todo{dataset, multiple clusters?}.
After the representatives were selected, the percentage of coverage by each representative was calculated and plotted into graph.


\subsection{Results}

\begin{figure}[H]
  \includegraphics[width=\linewidth]{img/exp5.png}
  \caption{\cbl Average fit of representatives in one cluster}
  \label{img:exp5}
\end{figure}

\subsection{Conclusion}
On average, the $\delta$-Medoids algorithm covers the cluster better with each added representative.
In the random selection, there is the possibility of selecting either the border cases or two points with overlapping neighborhoods.
This can occur even more often in data, that do not have standard distribution and can be denser in one part of a cluster than the other.

\setsecnumdepth{part}

%========================================CHAPTER==================================

\chapter{Conclusion}
Literature in both Network Anomaly Detection field and representative selection was studied.
Approaches and algorithms thus found were combined and implemented in prototype program.
Both general clustering datasets and specific datasets from security field were tested and compared in the prototype.

For testing the selected method datasets were created and gathered from the internet to create a diverse testbed for comparing algorithms.
From the selected algorithms both $\delta$-Medoids One Shot and $\delta$-Medoids outperformed Random selection of representatives, which is the current solution of representatives selection in the framework.
The improvement was at least by 1\% in each dataset, while average improvement was 2\%. \todo{Double check these numbers.}

\todo{Fine-tune the algorithms for the NBAD field and write here about it if it works.}

If compared by the requirements from the definition of representative selection problem as we approach it, the following can be concluded.
For fast representative selection that does result in more accurate result than selecting samples randomly, $\delta$-Medoids One Shot algorithm should be used.
If the speed is not of great concern or the datastets have lower tens of thousands of samples, full version of $\delta$ medoids yields the best results.

\bibliographystyle{iso690}
\bibliography{hlavac-thesis.bib}

\setsecnumdepth{all}
\appendix

\chapter{Abreviations}
% \printglossaries
\begin{description}
%VH I changed the order of NBAD and NIDS to keep alphabetical order
    \item[IDS]  Intrusion Detection System
	\item[NBAD] Network-Based Anomaly Detection
	\item[NIDS] Intrusion Detection System
\end{description}


\chapter{Contents of enclosed CD}

%change appropriately

\begin{figure}
	\dirtree{%
		.1 readme.txt\DTcomment{the file with CD contents description}.
		.1 exe\DTcomment{the directory with executables}.
		.1 src\DTcomment{the directory of source codes}.
		.2 wbdcm\DTcomment{implementation sources}.
		.2 thesis\DTcomment{the directory of \LaTeX{} source codes of the thesis}.
		.1 text\DTcomment{the thesis text directory}.
		.2 thesis.pdf\DTcomment{the thesis text in PDF format}.
		.2 thesis.ps\DTcomment{the thesis text in PS format}.
	}
\end{figure}

\end{document}
