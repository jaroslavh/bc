{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blobs in 2D Datasets Notebook\n",
    "\n",
    "Testing my algorithms on 2D generated datasets with centers approaching together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from scipy.spatial import distance\n",
    "\n",
    "from bc_utils import TestDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating datasets and saving them to pandas dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_deviations = np.array(range(1, 7), dtype=np.float32) / 10\n",
    "dataset_dfs = []\n",
    "for std_deviation in cluster_deviations:\n",
    "    X, y = make_blobs(n_samples=250, centers=2, cluster_std=std_deviation,\n",
    "                      n_features=2) #center 1 priblizovat...\n",
    "    new_dataframe = pd.DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "    dataset_dfs.append(new_dataframe)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing plots of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "colors = {0:'red', 1:'blue'}\n",
    "for df in dataset_dfs:\n",
    "    fix, ax = pyplot.subplots()\n",
    "    grouped = df.groupby('label')\n",
    "    for key, group in grouped:\n",
    "        group.plot(ax=ax, kind='scatter', x='x', y='y',\n",
    "                   label=key, color=colors[key])\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separating datasets to learn and test parts. Using own class TestDataset from utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset_dfs[0]      \n",
    "\n",
    "#creating dataset for testing algorithms\n",
    "learning_dataset = TestDf(dataset)\n",
    "\n",
    "learning_dataset.show_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_datasets = []\n",
    "for dataset in dataset_dfs:\n",
    "    learning_datasets.append(TestDf(dataset))\n",
    "    \n",
    "for ldataset in learning_datasets:\n",
    "    ldataset.show_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# shows histograms based on distance from first point in dataset\n",
    "def training_histogram(in_df):\n",
    "    base_point = in_df.values[0]\n",
    "\n",
    "    vals = in_df.values\n",
    "    histogram_data = []\n",
    "    for item in vals:\n",
    "        histogram_data.append(distance.euclidean(\n",
    "            [base_point[1], base_point[2]],\n",
    "            [item[1], item[2]]))\n",
    "\n",
    "    #for i in range(10, 60, 10):\n",
    "    bins = len(histogram_data) / 3\n",
    "    pyplot.hist(histogram_data, bins, facecolor='blue', alpha=0.5)\n",
    "    pyplot.show()\n",
    "    \n",
    "    mean_distance = np.mean(histogram_data)\n",
    "    print(\"Mean distance for this dataset: \" + str(mean_distance) + \"\\n\")\n",
    "    return mean_distance\n",
    "\n",
    "# finding representation of a cluster with random selection\n",
    "# pandas.DataFrame in_df\n",
    "# float delta\n",
    "def randomSelection(in_df, delta):\n",
    "    \n",
    "    medoid_indexes = []\n",
    "    index_list = list(in_df.index.values)\n",
    "    \n",
    "    while (index_list):\n",
    "        #print(\"Index length: \" + str(len(index_list)))\n",
    "        # select first unvisited index\n",
    "        ind = in_df.loc[index_list[0]].name\n",
    "        #print(\"Selected index for this run: \" + str(ind))\n",
    "        ref_point = [in_df.loc[ind].values[1], in_df.loc[ind].values[2]]\n",
    "        rem_indexes = []\n",
    "        #print(\"Lenght of in_df: \" + str(len(in_df)))\n",
    "        \n",
    "        # find indexes to drop from dataframe\n",
    "        for index, row in in_df.iterrows():\n",
    "            if distance.euclidean([row['x'], row['y']], ref_point) <= delta:\n",
    "                rem_indexes.append(index)\n",
    "        \n",
    "        index_list = [item for item in index_list if item not in rem_indexes]\n",
    "        #print(\"Droping these indexes: \" + str(rem_indexes))\n",
    "        rem_indexes.remove(ind)\n",
    "        medoid_indexes.append(ind)\n",
    "        \n",
    "        in_df = in_df.drop(rem_indexes)\n",
    "        #print(\"These indexes remain in in_df: \" + str(in_df.index.values))\n",
    "        \n",
    "    return in_df\n",
    "            \n",
    "\n",
    "    \n",
    "medoid_lst = []    \n",
    "for df in learning_datasets: #[0:1]:\n",
    "    \n",
    "    print(\"Datagram for 0\")\n",
    "    mean_0 = training_histogram(df.train_df.loc[df.train_df[\"label\"] == 0])\n",
    "    medoid_0 = randomSelection(df.train_df.loc[df.train_df[\"label\"] == 0], mean_0)\n",
    "    \n",
    "    print(\"Datagram for 1\")\n",
    "    mean_1 = training_histogram(df.train_df.loc[df.train_df[\"label\"] == 1])\n",
    "    medoid_1 = randomSelection(df.train_df.loc[df.train_df[\"label\"] == 1], mean_1)\n",
    "\n",
    "    medoid_lst.append([df, pd.concat([medoid_0, medoid_1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to classify points if they fit to the right dataset by KNeighbors algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyPoints(test):\n",
    "\n",
    "    X_train = test[0].test_df.iloc[:, 1:].values\n",
    "    y_train = test[0].test_df.iloc[:, 0].values\n",
    "    X_test = test[1].iloc[:, 1:].values\n",
    "    y_test = test[1].iloc[:, 0].values\n",
    "\n",
    "    from sklearn.preprocessing import StandardScaler  \n",
    "    scaler = StandardScaler()  \n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    X_train = scaler.transform(X_train)  \n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    from sklearn.neighbors import KNeighborsClassifier  \n",
    "    classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = classifier.predict(X_test)\n",
    "\n",
    "    from sklearn.metrics import classification_report, confusion_matrix  \n",
    "    print(confusion_matrix(y_test, y_pred))  \n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "for dataset in medoid_lst:\n",
    "    classifyPoints(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(learning_datasets[0].train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
